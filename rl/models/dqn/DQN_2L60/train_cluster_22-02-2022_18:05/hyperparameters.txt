Hyperparameters at training time:

    network_name = DQN_2L60
    batch_size = 128
    gamma = 0.999
    eps_start = 0.9
    eps_end = 0.05
    eps_decay = 1000
    target_update = 100
    num_episodes = 10000
    max_episode_steps = 100
    memory_replay = 10000
    test_freq = 1000
    save_freq = 500
    
Simulation c++ settings:

Table of reward settings:
Name                 Reward  Done  Trigger Min     Max     Ovrsht 
step_num             -0.010  0     1      
lifted               0.005   0     1      
oob                  0.000   1     1      
dropped              0.000   0     10000  
target_height        1.000   1     1      
exceed_limits        -0.100  0     1      
object_contact       0.005   0     1      
object_stable        1.000   1     1      
exceed_axial         -0.050  0     1       2.0     6.0     -1.0   
exceed_lateral       -0.050  0     1       4.0     6.0     -1.0   
exceed_palm          -0.050  0     1       6.0     10.0    -1.0   
palm_force           0.050   0     1       1.0     3.0     6.0    

Other settings:
gauge_read_rate_hz = 10.0
lift_distance = 0.001
oob_distance = 0.075
height_target = 0.01
stable_finger_force = 1.0
stable_palm_force = 2.0
max_timeouts = 10
action_motor_steps = 100
action_base_translation = 0.002
max_action_steps = 200
paired_motor_X_step = True
render_on_step = False
use_settling = False
use_render_delay = False
render_delay = 0.5
